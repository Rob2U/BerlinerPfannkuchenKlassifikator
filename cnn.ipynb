{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arm64\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "print(platform.machine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, labels, data, transform=None, target_transform=None):\n",
    "        self.img_labels = torch.from_numpy(labels)\n",
    "        self.img_labels = self.img_labels.to(device)\n",
    "        \n",
    "        data = np.moveaxis(data, 3, 1)\n",
    "        self.img_data = torch.from_numpy(data)\n",
    "        self.img_data = self.img_data.to(device)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        #return len(self.img_labels)\n",
    "        return len(self.img_labels)*50\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.img_labels)\n",
    "        image = self.img_data[idx]\n",
    "        \n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list()\n",
    "y_train = list()\n",
    "\n",
    "for file in pathlib.Path(\"/Users/robert/Documents/RepositorysLocal/BerlinerPfannkuchenKlassifikatior/BerlinerPfannkuchenKlassifikator/data/pfannkuchen\").iterdir():\n",
    "    if file.is_file() and file.suffix == \".bmp\":\n",
    "        pic = Image.open(\"/Users/robert/Documents/RepositorysLocal/BerlinerPfannkuchenKlassifikatior/BerlinerPfannkuchenKlassifikator/data/pfannkuchen/\" + file.name)\n",
    "        pix = np.asarray(pic)\n",
    "        pix = pix / 255.0\n",
    "        pix = pix.astype(np.float32)\n",
    "        \n",
    "        X_train.append(pix)\n",
    "        y_train.append(1)\n",
    "\n",
    "length = len(X_train)\n",
    "\n",
    "for file in pathlib.Path(\"/Users/robert/Documents/RepositorysLocal/BerlinerPfannkuchenKlassifikatior/BerlinerPfannkuchenKlassifikator/data/berliner\").iterdir():\n",
    "    if file.is_file() and file.suffix == \".bmp\":\n",
    "        pic = Image.open(\"/Users/robert/Documents/RepositorysLocal/BerlinerPfannkuchenKlassifikatior/BerlinerPfannkuchenKlassifikator/data/berliner/\" + file.name)\n",
    "        pix = np.asarray(pic)\n",
    "        pix = pix / 255.0\n",
    "        pix = pix.astype(np.float32)\n",
    "        \n",
    "        \n",
    "        X_train.append(pix)\n",
    "        y_train.append(0)\n",
    "        if len(X_train) >= length*2:\n",
    "            break\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "X_test = X_train[0:65]\n",
    "y_test = y_train[0:65]\n",
    "\n",
    "X_test = np.concatenate([X_test, X_train[len(X_train)-65:len(X_train)]])\n",
    "y_test = np.concatenate([y_test, y_train[len(y_train)-65:len(y_train)]])\n",
    "X_train = X_train[65:len(X_train)-65]\n",
    "y_train = y_train[65:len(y_train)-65]\n",
    "\n",
    "trainset = ImageDataset(y_train, X_train, transform=transforms.Compose([\n",
    "    #transforms.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
    "    transforms.RandomRotation(degrees=(0, 45)),\n",
    "    transforms.RandomResizedCrop(112, scale=(0.7,1.0), ratio=(0.7,1.33))\n",
    "]))\n",
    "\n",
    "\"\"\"\n",
    "data, lbl = trainset.__getitem__(0)\n",
    "data = np.asarray(data)\n",
    "data = np.moveaxis(data, 0, 2)\n",
    "print(data.shape)\n",
    "\n",
    "plt.imshow(data) \n",
    "plt.show()\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(14*14*128, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "\n",
    "net = ConvNet()\n",
    "net.to(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.873:  13%|█▎        | 132/1040 [00:31<03:28,  4.35it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Fix random seed to get reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size=25\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "losses = []\n",
    "running_loss = []\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    # EXTRA: get a nice progress bar visualization\n",
    "    train_progress  = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", mininterval=0.3)\n",
    "    for i, data in enumerate(train_progress, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        running_loss .append(loss.item())\n",
    "        losses.append(loss.item())\n",
    "        train_progress.set_description(f\"Epoch: {epoch+1}, loss: {loss.item():.3}\")\n",
    "    \n",
    "    print(f\"------ Epoch {epoch+1} ------\")\n",
    "    print(f\"Loss: {sum(running_loss)/(len(running_loss))}\")\n",
    "    running_loss = []\n",
    "\n",
    "print('Finished Training')\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4848, 0.5152]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = \"/Users/robert/Documents/RepositorysLocal/BerlinerPfannkuchenKlassifikatior/BerlinerPfannkuchenKlassifikator/8e670e4afba12afb3a87c4b11598e6e11d1380a7.bmp\"\n",
    "test_pic = Image.open(test_path)\n",
    "test_data = np.asarray(test_pic)\n",
    "test_data = (test_data / 255.0)\n",
    "test_data = test_data.astype(np.float32)\n",
    "test_data = test_data.reshape(-1, 3 , 224, 224)\n",
    "test_data = torch.from_numpy(test_data)\n",
    "\n",
    "prediction = net(test_data)\n",
    "prediction_loss = criterion(prediction, torch.tensor([0]))\n",
    "prediction_loss\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 112, 112, 3)\n",
      "[LibLinear]iter  1 act 2.801e+02 pre 2.460e+02 delta 1.979e+00 f 3.604e+02 |g| 9.867e+02 CG  21\n",
      "iter  2 act 4.864e+01 pre 3.940e+01 delta 1.979e+00 f 8.031e+01 |g| 2.509e+02 CG  20\n",
      "iter  3 act 1.326e+01 pre 1.072e+01 delta 1.979e+00 f 3.167e+01 |g| 9.121e+01 CG  18\n",
      "iter  4 act 3.579e+00 pre 2.961e+00 delta 1.979e+00 f 1.841e+01 |g| 3.255e+01 CG  18\n",
      "iter  5 act 7.394e-01 pre 6.256e-01 delta 1.979e+00 f 1.483e+01 |g| 1.136e+01 CG  18\n",
      "iter  6 act 1.692e-01 pre 1.492e-01 delta 1.979e+00 f 1.410e+01 |g| 3.723e+00 CG  24\n",
      "iter  7 act 1.630e-02 pre 1.520e-02 delta 1.979e+00 f 1.393e+01 |g| 1.203e+00 CG  24\n",
      "iter  8 act 5.257e-04 pre 5.232e-04 delta 1.979e+00 f 1.391e+01 |g| 2.022e-01 CG  25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;ovr&#x27;, solver=&#x27;liblinear&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;ovr&#x27;, solver=&#x27;liblinear&#x27;, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='ovr', solver='liblinear', verbose=1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from torchvision.transforms import RandomResizedCrop\n",
    "\n",
    "\"\"\"\n",
    "trainset = ImageDataset(y_train, X_train, transform=transforms.Compose([\n",
    "    #RandomResizedCrop(112, scale=(0.7,1.0), ratio=(0.5,2))\n",
    "]))\n",
    "ml_dataloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "impr_trainset_data = []\n",
    "impr_trainset_labels = []\n",
    "for batch in ml_dataloader:\n",
    "    data, lbl = batch\n",
    "    lbl = np.asarray(lbl)[0]\n",
    "    data = np.asarray(data)\n",
    "    data = np.moveaxis(data, 0, 2)\n",
    "    impr_trainset_data.append(data)\n",
    "    impr_trainset_labels.append(lbl)\n",
    "\n",
    "impr_trainset_labels = np.asarray(impr_trainset_labels)    \n",
    "impr_trainset_data = np.asarray(impr_trainset_data)\n",
    "impr_trainset_data = impr_trainset_data.reshape(-1, 3 * 112 * 112)\n",
    "\"\"\";\n",
    "impr_trainset_data = X_train.reshape(-1, 3 * 112 * 112)\n",
    "impr_trainset_labels = y_train\n",
    "\n",
    "model = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear', verbose=1)\n",
    "model.fit(impr_trainset_data, impr_trainset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/Users/robert/Documents/RepositorysLocal/BerlinerPfannkuchenKlassifikatior/BerlinerPfannkuchenKlassifikator/ccd1103058108dca8975f3aa0e15024118d43058.bmp\"\n",
    "test_path = \"/Users/robert/Documents/RepositorysLocal/BerlinerPfannkuchenKlassifikatior/BerlinerPfannkuchenKlassifikator/5d218a52d5657d1b40909b8805944b3f83b29c8d.bmp\"\n",
    "test_pic = Image.open(test_path)\n",
    "test_data = np.asarray(test_pic)\n",
    "test_data = (test_data / 255.0)\n",
    "test_data = test_data.astype(np.float32)\n",
    "test_data = test_data.reshape(-1, 3 , 112, 112)\n",
    "test_data = torch.from_numpy(test_data)\n",
    "test_data = test_data.reshape(-1, 3 * 112 * 112)\n",
    "\n",
    "pred = model.predict(test_data)\n",
    "pred\n",
    "\n",
    "print(f'Actual value from test data is 0 and corresponding image is as below')\n",
    "plt.matshow(test_data.reshape((112,112,3))) \n",
    "plt.show()\n",
    "print(f'Predicted value from test data is {pred[0]} and corresponding image is as below')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i in range(0,len(X_test)):\n",
    "    data = X_test[i]\n",
    "    data = data.reshape(-1, 112*112*3)\n",
    "    label = y_test[i]\n",
    "    pred = model.predict(data)\n",
    "    \n",
    "    accs.append(1 - np.abs(label - pred))\n",
    "    #print(pred)\n",
    "    #plt.matshow(data.reshape((112,112,3))) \n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "accuracy = sum(accs)*(1/len(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66923077]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6da7fd945713e64da6cbf98a618bc2c3ff6cb6697c7e79e8c184f718774efc42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
